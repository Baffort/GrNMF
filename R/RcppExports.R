# This file was generated by Rcpp::compileAttributes
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' A Network Constrained Version of Non-negative Matrix Factorization.
#' 
#' \code{grnmf} is an R/C++ implementation of the algorithm described in "Non-negative Matrix Factorization on Manifold", and modified slightly as 
#'   described in Hofree et al 2013 to put the constraint on features rather than samples.
#' @useDynLib GrNMF
#' @param Xr an p by n non-negative numeric matrix with each column being a sample, and each row corresponding to a feature.
#' @param Wr a symmetric numeric matrix (p by p) with 1 iff x_i,j and x_i',j' are nearest neighbors by some 
#'   graph distance metric, and 0 otherwise. You decide/compute this in advance, 11 is a good neighbor threshold from Hofree et al 2013.
#' @param k the number of inner dimensions (reduced features, or 'rank') to use for the NMF algorithm.
#' @param n_iter the number of optimization loop iterations.
#' @param lambda_multiple the scalar weight applied to the graphical constaint term, or the multiple applied to the update term
#' @param converge threshold for convergence test for early termination. Negative values deactivate this feature. 
#' @param dynamic_lambda Should we use the dynamic lambda updating scheme of Hofree et al? This makes 
#'   lambda_multiple the multiple applied to the dynamic updating term rather than the lambda that is applied to 
#'   directly to the network influence term on the GrNMF objective function.
#' @return a list of \code{U}, 
#'   (sometimes called \code{W}) and \code{V} 
#'   (sometimes called \code{H}) for the NNLS fit.
#'   \code{Max.iter} which stores the maximum iteration before NMF converged 
#'   Additionally the standard NMF objective 
#'   function fit is returned as \code{ObjectiveFitNMF}.
#'   The full GrNMF objective function score is retured as
#'   \code{ObjectiveFitGrNMF}
#' @references Cai, D., He, X., Wu, X., & Han, J. (2008). Non-negative Matrix Factorization on Manifold. 2008 Eighth IEEE International Conference on Data Mining (ICDM), 63-72. doi:10.1109/ICDM.2008.57
#' @references Xu, W., Liu, X., & Gong, Y. (2003). Document clustering based on non-negative matrix factorization. the 26th annual international ACM SIGIR conference (pp. 267-273). New York, New York, USA: ACM. doi:10.1145/860435.860485
#' @references Hofree, M., Shen, J. P., Carter, H., Gross, A., & Ideker, T. (2013). Network-based stratification of tumor mutations. Nature Methods, 10(11), 1108-1115. doi:10.1038/nmeth.2651
#' @export
#' @examples
#' 
#' # The following is adapted from the NMF package vignette
#' # and uses their functions to set up demo data, and as an NMF implementation
#' # comparison (see http://cran.r-project.org/package=NMF)
#' 
#' # First, generate a synthetic dataset with known classes: 100 features, 23 samples (10+5+8)
#' 
#' library(NMF)
#' set.seed(1234)
#' p <- 100; counts <- c(10, 5, 8);
#' n <- sum(counts)
#' x <- syntheticNMF(p, counts)
#' dim(x)
#' # build the true cluster membership
#' groups <- unlist(mapply(rep, seq(counts), counts))
#' # run on a data.frame
#' set.seed(10)
#' system.time(res <- nmf(data.frame(x), 3, nmfAlgorithm("lee"), "random"))
#' 
#' 
#' # Now do the same for GrNMF, and let's compare how close they are
#' d<-as.matrix(dist(x))
#' nn<-5 # 5 nearest neighbors for adjacency graph construction
#' adj <- apply(d, 1, function(drow){
#'   cut <- sort(drow,partial=nn+1)[nn+1] #this is the nn+1 smallest
#'   ifelse(drow<cut,1,0)
#' })
#' adj <- (adj | t(adj)) + 0 #make it symmetric, 
#'   # (the + 0 turns it back to numeric 0/1)
#' sum(adj!=t(adj)) == 0
#' 
#' set.seed(10)
#' 
#' # turn off the graphical part for this comparison
#' # by setting lambda to 0
#' system.time(res2<-grnmf(x,adj,lambda_multiple=0,k=3))
#' 
#' ##
#' # now we can compare the two fits to the NMF
#' # objective function
#' 
#' # first from the NMF package
#' norm(as.matrix(data.frame(x)- (res@@fit@@W %*% res@@fit@@H)),'F')
#' # and next get the NMF objective function fit from grnmf
#' res2$ObjectiveFitNMF
#' 
#' #it should be the same as the GrNMF fit because lambda=0
#' res2$ObjectiveFitNMF == res2$ObjectiveFitGrNMF
#' 
#' # Now test out but use our demo graph
#' set.seed(10)
#' system.time(res3<-grnmf(x, adj, k=3))
#' res3$ObjectiveFitNMF
#' res3$ObjectiveFitGrNMF
#' 
#' 
#' ##
#' # Now do some simple clustering with kmeans
#' 
#' # function to check if our clusters (which are assigned an arbitrary label)
#' # are the same
#' check_fit <- function(cluster){
#'   c1 = cluster[1:counts[1]]
#'   c2 = cluster[(counts[1]+1):(counts[1]+counts[2])]
#'   c3 = cluster[(counts[1]+counts[2]+1):sum(counts)]
#'   c1m = median(c1)
#'   c2m = median(c2)
#'   c3m = median(c3)
#'   if(length(unique(c(c1m,c2m,c3m))) != 3){
#'     cat("Warning, the following error estimate will be",
#'     "incorrect, there are a large number of prediction errors!\n")
#'   }
#'   correct <- ifelse(c(c1-c1m,
#'     c2-c2m,
#'     c3-c3m
#'   ) == 0, 1, 0)
#'   correct
#' }
#' 
#' # on raw X matrix
#' fit_ori <- kmeans(t(x),3)
#' sum(check_fit(fit_ori$cluster))/n
#' 
#' # on standard NMF matrix
#' fit_nmf <- kmeans(t(res@@fit@@H),3)
#' sum(check_fit(fit_nmf$cluster))/n
#' 
#' # our GrNMF should be equivalent when lambda=0
#' fit_nmf2 <- kmeans(res2$V,3)
#' sum(check_fit(fit_nmf2$cluster))/n
#' 
#' # our GrNMF based clusters
#' fit_grnmf <- kmeans(res3$V,3)
#' sum(check_fit(fit_grnmf$cluster))/n
#' 
grnmf <- function(Xr, Wr, k = 5L, lambda_multiple = 1L, n_iter = 1000L, converge = 1e-6, dynamic_lambda = TRUE) {
    .Call('GrNMF_grnmf', PACKAGE = 'GrNMF', Xr, Wr, k, lambda_multiple, n_iter, converge, dynamic_lambda)
}

